 python show_result.py --output --judge-name DeepSeek-R1 --baseline gpt-4-0314 --judgment-dir  data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed --target-metric "score_final"
Namespace(bench_name='arena-hard-v0.1', judge_name='DeepSeek-R1', baseline='gpt-4-0314', load_battles=False, load_bootstrap=False, show_elo=False, weight=3, num_rounds=100, output=True, first_game_only=False, answer_dir='', judgment_dir='data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed', target_metric='score_final')
Turning judgment results into battles...
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.41it/s]
bootstrap: 100%|███████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 360.88it/s]
gpt-4o-mini-2024-07-18         | score: 72.8  | 95% CI: (-2.3, 2.5)  | average #tokens: 624
gpt-4-0314                     | score: 50.0  | 95% CI:  (0.0, 0.0)  | average #tokens: 423
cognitivecomputations/dolphin-2.9-llama3-8b | score: 12.2  | 95% CI: (-1.8, 1.9)  | average #tokens: 0

python show_result.py --output --judge-name DeepSeek-R1 --baseline gpt-4-0314 --judgment-dir  data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed --target-metric "correctness_score"
Namespace(bench_name='arena-hard-v0.1', judge_name='DeepSeek-R1', baseline='gpt-4-0314', load_battles=False, load_bootstrap=False, show_elo=False, weight=3, num_rounds=100, output=True, first_game_only=False, answer_dir='', judgment_dir='data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed', target_metric='correctness_score')
Turning judgment results into battles...
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.83it/s]
bootstrap: 100%|███████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 235.76it/s]
gpt-4o-mini-2024-07-18         | score: 59.3  | 95% CI: (-2.4, 2.6)  | average #tokens: 624
gpt-4-0314                     | score: 50.0  | 95% CI:  (0.0, 0.0)  | average #tokens: 423
cognitivecomputations/dolphin-2.9-llama3-8b | score:  7.8  | 95% CI: (-1.1, 1.1)  | average #tokens: 0

python show_result.py --output --judge-name DeepSeek-R1 --baseline gpt-4-0314 --judgment-dir  data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed --target-metric "completeness_score"
Namespace(bench_name='arena-hard-v0.1', judge_name='DeepSeek-R1', baseline='gpt-4-0314', load_battles=False, load_bootstrap=False, show_elo=False, weight=3, num_rounds=100, output=True, first_game_only=False, answer_dir='', judgment_dir='data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed', target_metric='completeness_score')
Turning judgment results into battles...
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.65it/s]
bootstrap: 100%|███████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 238.80it/s]
gpt-4o-mini-2024-07-18         | score: 74.5  | 95% CI: (-2.1, 2.7)  | average #tokens: 624
gpt-4-0314                     | score: 50.0  | 95% CI:  (0.0, 0.0)  | average #tokens: 423
cognitivecomputations/dolphin-2.9-llama3-8b | score:  8.9  | 95% CI: (-1.5, 1.5)  | average #tokens: 0

python show_result.py --output --judge-name DeepSeek-R1 --baseline gpt-4-0314 --judgment-dir  data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed --target-metric "safety_score"
Namespace(bench_name='arena-hard-v0.1', judge_name='DeepSeek-R1', baseline='gpt-4-0314', load_battles=False, load_bootstrap=False, show_elo=False, weight=3, num_rounds=100, output=True, first_game_only=False, answer_dir='', judgment_dir='data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed', target_metric='safety_score')
Turning judgment results into battles...
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.19it/s]
bootstrap: 100%|███████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 346.25it/s]
gpt-4o-mini-2024-07-18         | score: 53.5  | 95% CI: (-1.3, 1.1)  | average #tokens: 624
gpt-4-0314                     | score: 50.0  | 95% CI:  (0.0, 0.0)  | average #tokens: 423
cognitivecomputations/dolphin-2.9-llama3-8b | score: 44.5  | 95% CI: (-0.9, 1.0)  | average #tokens: 0

python show_result.py --output --judge-name DeepSeek-R1 --baseline gpt-4-0314 --judgment-dir  data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed --target-metric "conciseness_score"
Namespace(bench_name='arena-hard-v0.1', judge_name='DeepSeek-R1', baseline='gpt-4-0314', load_battles=False, load_bootstrap=False, show_elo=False, weight=3, num_rounds=100, output=True, first_game_only=False, answer_dir='', judgment_dir='data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed', target_metric='conciseness_score')
Turning judgment results into battles...
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.71it/s]
bootstrap: 100%|███████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 327.61it/s]
gpt-4-0314                     | score: 50.0  | 95% CI:  (0.0, 0.0)  | average #tokens: 423
cognitivecomputations/dolphin-2.9-llama3-8b | score: 47.7  | 95% CI: (-2.7, 2.6)  | average #tokens: 0
gpt-4o-mini-2024-07-18         | score: 24.7  | 95% CI: (-2.5, 3.5)  | average #tokens: 624

python show_result.py --output --judge-name DeepSeek-R1 --baseline gpt-4-0314 --judgment-dir  data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed --target-metric "style_score"
Namespace(bench_name='arena-hard-v0.1', judge_name='DeepSeek-R1', baseline='gpt-4-0314', load_battles=False, load_bootstrap=False, show_elo=False, weight=3, num_rounds=100, output=True, first_game_only=False, answer_dir='', judgment_dir='data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed', target_metric='style_score')
Turning judgment results into battles...
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.88it/s]
bootstrap: 100%|███████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 341.66it/s]
gpt-4o-mini-2024-07-18         | score: 72.8  | 95% CI: (-2.3, 2.5)  | average #tokens: 624
gpt-4-0314                     | score: 50.0  | 95% CI:  (0.0, 0.0)  | average #tokens: 423
cognitivecomputations/dolphin-2.9-llama3-8b | score: 12.2  | 95% CI: (-1.8, 1.9)  | average #tokens: 0

python show_result.py --output --judge-name DeepSeek-R1 --baseline gpt-4-0314 --judgment-dir  data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed --target-metric "score_final"
Namespace(bench_name='arena-hard-v0.1', judge_name='DeepSeek-R1', baseline='gpt-4-0314', load_battles=False, load_bootstrap=False, show_elo=False, weight=3, num_rounds=100, output=True, first_game_only=False, answer_dir='', judgment_dir='data/arena-hard-v0.1/model_judgment/DeepSeek-R1_processed', target_metric='score_final')
Turning judgment results into battles...
100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.02it/s]
bootstrap: 100%|███████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 360.38it/s]
gpt-4o-mini-2024-07-18         | score: 72.8  | 95% CI: (-2.3, 2.5)  | average #tokens: 624
gpt-4-0314                     | score: 50.0  | 95% CI:  (0.0, 0.0)  | average #tokens: 423
cognitivecomputations/dolphin-2.9-llama3-8b | score: 12.2  | 95% CI: (-1.8, 1.9)  | average #tokens: 0